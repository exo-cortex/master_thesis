\chapter{one}

\section{Introduction}

	Reservoir Computing encompasses the field of machine learning in which the capacity to store and compute transform data is investigated in a wide variety of dynamical systems (reservoirs). Reservoir Computing (RC) is particularly interesting as computations can be performed by the physical systems directly. Today "classical" computers of the van-Neumann-type (or more generally of the Turing-machine-type) need rely on the existence of a digital space in which everything can be represented by a combination of discrete values ($0$ and $1$). In order to establish such a computing space in our analogue world  
	
	in the sense of the Turing-machine as a model is more like a virtual space created in our physical space. Within this virtual space everything is defined only in discrete units of "0" and "1". In order to create such a virtual space the usual unpredictability that inhabits the scales in which modern computer circuits exist in has to be tamed in order to create this virtual computing space. As the scales of modern transistors shrink they rapidly approach the scales in which quantum effects become problematic. The maintaining of this virtual space of $0$ and $1$ in which all our computations are performed is increasingly difficult. Modern CPU manufacturing has to take into account many error compensation algorithms in. Simultaneously manufacturing is becoming more challenging as well since the scales of modern transistors are so small the light sources needed for lithography are becoming rare.
	
	Reservoir computing is the circumventing of this virtual space of discrete values in order to perform computations directly in physical systems. A wide variety of systems can be used e.g. a literal bucket of water can act as a reservoir that performs computations \cite{FER03}. Albeit the most interesting applications lie in potential optical computers. RC offers a way of utilizing the highly complex dynamics of optical systems in order to perform computation on them. Optical Computers in the form of lasers appear to be an ideal application of RC, because of the timescales that laser dynamics.  Optical reservoir computers already perform classification tasks on very timescales unmatched by modern silicon electronics \cite{BRU13a}
	Another expected benefit would be the vastly lower power consumption.  
	

	In recent years reservoir computing has received a lot of attention as bridge between machine learning and physics. As the end of "Moore's Law" is slowly encroaching we are in the waning years of an age of staggering performance leaps in silicon electronic circuits. Reservoir Computing as a way of utilizing the much smaller timescales at which optical systems operate offer a way of building drastically faster computers. There
	
	\cite{SAN17a} overview-paper

	\cite{LAR12} i fischer.
	
	\cite{ROE18a} paper von Andr√©. (has normalization?)
	
	\cite{JAE01} let's not train the networks.
	
	\cite{APP11} - original paper introducint the delay-based reservoir approach.
	
	\cite{ANT19} ? lesen!
	
	\cite{STE20} ? lesen
	
	\cite{ATI00} - NARMA10 task introduction.
	

\subsection{blabla}

Analyzing timeseries data is an important part of machine learning tasks. Many datasets can be regarded as a timeseries. Temperature, pressure and wind evolve over time and are fed into complicated climate prediction models in order to continue this timeseries into the future - predicting tomorrows weather patterns. The applications of extracting information from timeseries are vast. Predicting the stock market, driving an autonomous car based on a video stream and other vehicle metrics, recognizing types of heart arrhythmia or more general heart diseases and infections through analysis of electrocardiography by machine learning algorhithms (https://ieeexplore.ieee.org/abstract/document/7164783). Last, but not least: the most apparent machine learning application to date: voice recognition algorithms are predicting the meaning of audio information everytime we utter the magical phrase "Hi, Siri", "Ok, Google" or "Alexa...". 
It can also be argued that classical prediction tasks that do not involve time-conscious datasets like images fed into one directional feed forward networks 
